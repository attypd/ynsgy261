import requests
import re

# é¡¶çº§å¤§ä½¬åŠå…¬å…±èµ„æºæ± ï¼ˆè¿™äº›ä»“åº“é€šå¸¸åŒ…å«æ¸¯å°ã€æ˜Ÿå½±ã€æ¾è§†ç­‰ï¼‰
target_urls = [
    "https://raw.githubusercontent.com/fanmingming/live/main/tv/m3u/ipv6.m3u",
    "https://raw.githubusercontent.com/Guovin/TV/gd/output/result.txt",
    "https://raw.githubusercontent.com/yuanzl77/IPTV/main/live.txt",
    "https://raw.githubusercontent.com/ssili126/tv/main/itvlist.txt",
    "http://175.178.251.183:668/livedata/itvlist.txt"
]

# æ ¸å¿ƒå…³é”®è¯åº“ï¼šåŒ…å«ä½ æåˆ°çš„çˆ†è°·ã€å¤©æ˜ ã€æ¾è§†ç­‰
keywords = [
    "é¦™æ¸¯", "å°æ¹¾", "HK", "TW", "TVB", "ç¿¡ç¿ ", "å¤©æ˜ ", "çˆ†è°·", "æ˜Ÿå½±", 
    "CHC", "å‡¤å‡°", "æ¾è§†", "å½©è™¹", "Sugo", "Panas", "Cherry", "Honey"
]

def super_spider():
    collected_channels = []
    print("ğŸš€ å¯åŠ¨è¶…çº§çˆ¬è™«ï¼Œæ‰«æå…¨ç½‘é«˜ä»·å€¼æº...")

    for url in target_urls:
        try:
            r = requests.get(url, timeout=15)
            if r.status_code == 200:
                lines = r.text.split('\n')
                for line in lines:
                    # è¿‡æ»¤é€»è¾‘ï¼šåŒ…å«å…³é”®è¯ ä¸” åŒ…å«æœ‰æ•ˆ http é“¾æ¥
                    if any(k.lower() in line.lower() for k in keywords):
                        if "http" in line:
                            # ç»Ÿä¸€æ¸…æ´—æ ¼å¼ï¼Œç¡®ä¿ç›’å­èƒ½è¯†åˆ«
                            clean_line = line.strip().replace("\r", "")
                            collected_channels.append(clean_line)
        except:
            continue

    # å»é‡å¤„ç†
    result = list(set(collected_channels))
    
    # å†™å…¥ä¼ªè£…æ–‡ä»¶ v_data_88.txt
    with open("v_data_88.txt", "w", encoding="utf-8") as f:
        f.write("#EXTM3U\n")
        f.write("\n".join(result))
    
    print(f"âœ… ä»»åŠ¡å®Œæˆï¼ŒæˆåŠŸæ•è· {len(result)} ä¸ªé¢‘é“ï¼")

if __name__ == "__main__":
    super_spider()
